{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import firebase_admin\n",
    "from firebase_admin import credentials\n",
    "from firebase_admin import firestore\n",
    "from firebase_admin import auth"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add user input\n",
    "Fill in the variables in the cell below with the following information\n",
    "\n",
    "- `experiment` = name of experiment (outermost collection) in Firebase\n",
    "- `local_folder` = name of folder you want created locally and populated with data\n",
    "- `credential_path` = path to .json Firebase SDK service account key (explained [here](https://firebase.google.com/docs/admin/setup))\n",
    "- `firebase_project_name` = name of project in Firebase (click Project Overview in the Firebase console to see name)\n",
    "- `groups` = array of group names (as set by userGroup in src/utils.js and found as collections in the 'subject' document in your Firebase), each enclosed in single quotes and separated by commas\n",
    "- `rating_types` = array of rating types, each enclosed in single quotes and separated by commas (same as ratingTypes from src/utils.js)\n",
    "- `movie_names` = array of movie names, as written in Firebase stimuli table (no spaces), each enclosed in single quotes and separated by commas\n",
    "- `vid_lens` = dictionary of video durations, where each entry contains the movie name as key and the movie duration (in seconds) as value (e.g. {'movie1': 100, 'movie2': 150, 'movie3': 120}) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER VARIABLES (FILL IN HERE)\n",
    "experiment = 'EVrating'\n",
    "local_folder = 'data_20230624'\n",
    "credential_path = 'continuous-rater-28771-firebase-adminsdk-4zble-ffc5770709.json'\n",
    "firebase_project_name = 'continuous-rater'\n",
    "groups = ['MTurk Group'] # possible to only have on group, or can use multiple for organizational purposes\n",
    "\n",
    "rating_types =  ['Anger', 'Joy', 'Sadness', 'Suprise', 'Shame', 'Fear']\n",
    "movie_names = ['Aldan_Angry', 'Aldan_Ashamed', 'Aldan_Fear', 'Aldan_Happy', 'Aldan_Sad', 'Aldan_Surprised']\n",
    "vid_lens = {'Aldan_Angry': 53, 'Aldan_Ashamed': 68, 'Aldan_Fear': 53, 'Aldan_Happy': 53, 'Aldan_Sad': 64, 'Aldan_Surprised': 49}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up communication with Firebase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cred = credentials.Certificate(credential_path)\n",
    "firebase_admin.initialize_app(cred, {\n",
    "  'projectId': firebase_project_name,\n",
    "})\n",
    "\n",
    "db = firestore.client()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize data structures and directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize data structures\n",
    "subject_rating_dict = {}\n",
    "movie_q_count_dict = {}\n",
    "sub_to_mov = {}\n",
    "mov_to_sub = {}\n",
    "\n",
    "# initialize directory structure\n",
    "directories = []\n",
    "directories.append(f'{local_folder}/')\n",
    "directories.append(f'{local_folder}/Long/')\n",
    "directories.append(f'{local_folder}/Blanks/')\n",
    "directories.append(f'{local_folder}/Subjects/')\n",
    "directories.append(f'{local_folder}/Subjects/Incomplete/')\n",
    "directories.append(f'{local_folder}/Subjects/Groups/')\n",
    "directories.append(f'{local_folder}/Summary/')\n",
    "directories.append(f'{local_folder}/Ratings/')\n",
    "for movie in movie_names:\n",
    "    for rating in rating_types:\n",
    "        combo = movie + \"-\" + rating\n",
    "        movie_q_count_dict[combo] = 0\n",
    "        mov_to_sub[combo] = []\n",
    "        directories.append(f'{local_folder}/Ratings/{combo}/')\n",
    "        \n",
    "for directory in directories:\n",
    "    if not os.path.isdir(directory):\n",
    "        os.mkdir(directory) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create blank pandas dataframes of proper shape to fill in with rating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use provided movie durations to create blank dataframe of proper length for each movie\n",
    "blanks_path = f'{local_folder}/Blanks/'\n",
    "for movie in movie_names:\n",
    "    curr_vid_len = vid_lens[movie] + 3 # add buffer time\n",
    "    init_ratings = np.full(curr_vid_len, -1)\n",
    "    curr_df = pd.DataFrame({'rating': init_ratings})\n",
    "    curr_df.to_csv(os.path.join(blanks_path, f'{movie}.csv')) # write out to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read blank dataframes back in and store in dictionary\n",
    "blanks_path = f'{local_folder}/Blanks/'\n",
    "blank_pd_dict = {}\n",
    "directory_list = glob.glob(os.path.join(blanks_path, '*.csv'))\n",
    "for file in directory_list:\n",
    "    curr_df = pd.read_csv(file)\n",
    "    drop_df = curr_df.drop(labels='Unnamed: 0', axis=1)\n",
    "    movie = os.path.basename(file).split('.')[0]\n",
    "    blank_pd_dict[movie] = drop_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine which subjects completed task, and save subject info to .csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionDenied",
     "evalue": "403 Missing or insufficient permissions.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_MultiThreadedRendezvous\u001b[0m                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\google\\api_core\\grpc_helpers.py:162\u001b[0m, in \u001b[0;36m_wrap_stream_errors.<locals>.error_remapped_callable\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    161\u001b[0m     prefetch_first \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(callable_, \u001b[39m\"\u001b[39m\u001b[39m_prefetch_first_result_\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m--> 162\u001b[0m     \u001b[39mreturn\u001b[39;00m _StreamingResponseIterator(\n\u001b[0;32m    163\u001b[0m         result, prefetch_first_result\u001b[39m=\u001b[39;49mprefetch_first\n\u001b[0;32m    164\u001b[0m     )\n\u001b[0;32m    165\u001b[0m \u001b[39mexcept\u001b[39;00m grpc\u001b[39m.\u001b[39mRpcError \u001b[39mas\u001b[39;00m exc:\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\google\\api_core\\grpc_helpers.py:88\u001b[0m, in \u001b[0;36m_StreamingResponseIterator.__init__\u001b[1;34m(self, wrapped, prefetch_first_result)\u001b[0m\n\u001b[0;32m     87\u001b[0m     \u001b[39mif\u001b[39;00m prefetch_first_result:\n\u001b[1;32m---> 88\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stored_first_result \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_wrapped)\n\u001b[0;32m     89\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m     90\u001b[0m     \u001b[39m# It is possible the wrapped method isn't an iterable (a grpc.Call\u001b[39;00m\n\u001b[0;32m     91\u001b[0m     \u001b[39m# for instance). If this happens don't store the first result.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\grpc\\_channel.py:475\u001b[0m, in \u001b[0;36m_Rendezvous.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    474\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__next__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 475\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next()\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\grpc\\_channel.py:881\u001b[0m, in \u001b[0;36m_MultiThreadedRendezvous._next\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    880\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state\u001b[39m.\u001b[39mcode \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 881\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "\u001b[1;31m_MultiThreadedRendezvous\u001b[0m: <_MultiThreadedRendezvous of RPC that terminated with:\n\tstatus = StatusCode.PERMISSION_DENIED\n\tdetails = \"Missing or insufficient permissions.\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:142.250.180.10:443 {created_time:\"2023-06-25T09:27:52.29298882+00:00\", grpc_status:7, grpc_message:\"Missing or insufficient permissions.\"}\"\n>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mPermissionDenied\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\bernu\\Dropbox\\+Projects\\Empathic Agent\\continuous-rater\\continuous-rater-firebase.ipynb Cell 12\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/bernu/Dropbox/%2BProjects/Empathic%20Agent/continuous-rater/continuous-rater-firebase.ipynb#X14sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m group_collection \u001b[39m=\u001b[39m db\u001b[39m.\u001b[39mcollection(group_path)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/bernu/Dropbox/%2BProjects/Empathic%20Agent/continuous-rater/continuous-rater-firebase.ipynb#X14sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m#orig group_subs = group_collection.stream()\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/bernu/Dropbox/%2BProjects/Empathic%20Agent/continuous-rater/continuous-rater-firebase.ipynb#X14sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m group_subs \u001b[39m=\u001b[39m group_collection\u001b[39m.\u001b[39;49mget()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/bernu/Dropbox/%2BProjects/Empathic%20Agent/continuous-rater/continuous-rater-firebase.ipynb#X14sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mfor\u001b[39;00m sub \u001b[39min\u001b[39;00m group_subs:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/bernu/Dropbox/%2BProjects/Empathic%20Agent/continuous-rater/continuous-rater-firebase.ipynb#X14sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mhello\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\google\\cloud\\firestore_v1\\collection.py:186\u001b[0m, in \u001b[0;36mCollectionReference.get\u001b[1;34m(self, transaction, retry, timeout)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[39m\"\"\"Read the documents in this collection.\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \n\u001b[0;32m    165\u001b[0m \u001b[39mThis sends a ``RunQuery`` RPC and returns a list of documents\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[39m    list: The documents in this collection that match the query.\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    184\u001b[0m query, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prep_get_or_stream(retry, timeout)\n\u001b[1;32m--> 186\u001b[0m \u001b[39mreturn\u001b[39;00m query\u001b[39m.\u001b[39mget(transaction\u001b[39m=\u001b[39mtransaction, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\google\\cloud\\firestore_v1\\query.py:172\u001b[0m, in \u001b[0;36mQuery.get\u001b[1;34m(self, transaction, retry, timeout)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[39mif\u001b[39;00m is_limited_to_last:\n\u001b[0;32m    170\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mreversed\u001b[39m(\u001b[39mlist\u001b[39m(result))\n\u001b[1;32m--> 172\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39;49m(result)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\google\\cloud\\firestore_v1\\query.py:287\u001b[0m, in \u001b[0;36mQuery.stream\u001b[1;34m(self, transaction, retry, timeout)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstream\u001b[39m(\n\u001b[0;32m    252\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    253\u001b[0m     transaction\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    254\u001b[0m     retry: retries\u001b[39m.\u001b[39mRetry \u001b[39m=\u001b[39m gapic_v1\u001b[39m.\u001b[39mmethod\u001b[39m.\u001b[39mDEFAULT,\n\u001b[0;32m    255\u001b[0m     timeout: \u001b[39mfloat\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    256\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Generator[document\u001b[39m.\u001b[39mDocumentSnapshot, Any, \u001b[39mNone\u001b[39;00m]:\n\u001b[0;32m    257\u001b[0m     \u001b[39m\"\"\"Read the documents in the collection that match this query.\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \n\u001b[0;32m    259\u001b[0m \u001b[39m    This sends a ``RunQuery`` RPC and then returns an iterator which\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[39m        The next document that fulfills the query.\u001b[39;00m\n\u001b[0;32m    286\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 287\u001b[0m     response_iterator, expected_prefix \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_stream_iterator(\n\u001b[0;32m    288\u001b[0m         transaction,\n\u001b[0;32m    289\u001b[0m         retry,\n\u001b[0;32m    290\u001b[0m         timeout,\n\u001b[0;32m    291\u001b[0m     )\n\u001b[0;32m    293\u001b[0m     last_snapshot \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    295\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\google\\cloud\\firestore_v1\\query.py:221\u001b[0m, in \u001b[0;36mQuery._get_stream_iterator\u001b[1;34m(self, transaction, retry, timeout)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[39m\"\"\"Helper method for :meth:`stream`.\"\"\"\u001b[39;00m\n\u001b[0;32m    215\u001b[0m request, expected_prefix, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prep_stream(\n\u001b[0;32m    216\u001b[0m     transaction,\n\u001b[0;32m    217\u001b[0m     retry,\n\u001b[0;32m    218\u001b[0m     timeout,\n\u001b[0;32m    219\u001b[0m )\n\u001b[1;32m--> 221\u001b[0m response_iterator \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_client\u001b[39m.\u001b[39m_firestore_api\u001b[39m.\u001b[39mrun_query(\n\u001b[0;32m    222\u001b[0m     request\u001b[39m=\u001b[39mrequest,\n\u001b[0;32m    223\u001b[0m     metadata\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_client\u001b[39m.\u001b[39m_rpc_metadata,\n\u001b[0;32m    224\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    225\u001b[0m )\n\u001b[0;32m    227\u001b[0m \u001b[39mreturn\u001b[39;00m response_iterator, expected_prefix\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\google\\cloud\\firestore_v1\\services\\firestore\\client.py:1309\u001b[0m, in \u001b[0;36mFirestoreClient.run_query\u001b[1;34m(self, request, retry, timeout, metadata)\u001b[0m\n\u001b[0;32m   1304\u001b[0m metadata \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(metadata) \u001b[39m+\u001b[39m (\n\u001b[0;32m   1305\u001b[0m     gapic_v1\u001b[39m.\u001b[39mrouting_header\u001b[39m.\u001b[39mto_grpc_metadata(((\u001b[39m\"\u001b[39m\u001b[39mparent\u001b[39m\u001b[39m\"\u001b[39m, request\u001b[39m.\u001b[39mparent),)),\n\u001b[0;32m   1306\u001b[0m )\n\u001b[0;32m   1308\u001b[0m \u001b[39m# Send the request.\u001b[39;00m\n\u001b[1;32m-> 1309\u001b[0m response \u001b[39m=\u001b[39m rpc(\n\u001b[0;32m   1310\u001b[0m     request,\n\u001b[0;32m   1311\u001b[0m     retry\u001b[39m=\u001b[39;49mretry,\n\u001b[0;32m   1312\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m   1313\u001b[0m     metadata\u001b[39m=\u001b[39;49mmetadata,\n\u001b[0;32m   1314\u001b[0m )\n\u001b[0;32m   1316\u001b[0m \u001b[39m# Done; return the response.\u001b[39;00m\n\u001b[0;32m   1317\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\google\\api_core\\gapic_v1\\method.py:113\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[1;34m(self, timeout, retry, *args, **kwargs)\u001b[0m\n\u001b[0;32m    110\u001b[0m     metadata\u001b[39m.\u001b[39mextend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_metadata)\n\u001b[0;32m    111\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mmetadata\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m metadata\n\u001b[1;32m--> 113\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped_func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\google\\api_core\\retry.py:349\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    345\u001b[0m target \u001b[39m=\u001b[39m functools\u001b[39m.\u001b[39mpartial(func, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    346\u001b[0m sleep_generator \u001b[39m=\u001b[39m exponential_sleep_generator(\n\u001b[0;32m    347\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_initial, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maximum, multiplier\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_multiplier\n\u001b[0;32m    348\u001b[0m )\n\u001b[1;32m--> 349\u001b[0m \u001b[39mreturn\u001b[39;00m retry_target(\n\u001b[0;32m    350\u001b[0m     target,\n\u001b[0;32m    351\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_predicate,\n\u001b[0;32m    352\u001b[0m     sleep_generator,\n\u001b[0;32m    353\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_timeout,\n\u001b[0;32m    354\u001b[0m     on_error\u001b[39m=\u001b[39;49mon_error,\n\u001b[0;32m    355\u001b[0m )\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\google\\api_core\\retry.py:191\u001b[0m, in \u001b[0;36mretry_target\u001b[1;34m(target, predicate, sleep_generator, timeout, on_error, **kwargs)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[39mfor\u001b[39;00m sleep \u001b[39min\u001b[39;00m sleep_generator:\n\u001b[0;32m    190\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 191\u001b[0m         \u001b[39mreturn\u001b[39;00m target()\n\u001b[0;32m    193\u001b[0m     \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m    194\u001b[0m     \u001b[39m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[0;32m    195\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\google\\api_core\\timeout.py:120\u001b[0m, in \u001b[0;36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    117\u001b[0m     \u001b[39m# Avoid setting negative timeout\u001b[39;00m\n\u001b[0;32m    118\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout \u001b[39m-\u001b[39m time_since_first_attempt)\n\u001b[1;32m--> 120\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\google\\api_core\\grpc_helpers.py:166\u001b[0m, in \u001b[0;36m_wrap_stream_errors.<locals>.error_remapped_callable\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    162\u001b[0m     \u001b[39mreturn\u001b[39;00m _StreamingResponseIterator(\n\u001b[0;32m    163\u001b[0m         result, prefetch_first_result\u001b[39m=\u001b[39mprefetch_first\n\u001b[0;32m    164\u001b[0m     )\n\u001b[0;32m    165\u001b[0m \u001b[39mexcept\u001b[39;00m grpc\u001b[39m.\u001b[39mRpcError \u001b[39mas\u001b[39;00m exc:\n\u001b[1;32m--> 166\u001b[0m     \u001b[39mraise\u001b[39;00m exceptions\u001b[39m.\u001b[39mfrom_grpc_error(exc) \u001b[39mfrom\u001b[39;00m \u001b[39mexc\u001b[39;00m\n",
      "\u001b[1;31mPermissionDenied\u001b[0m: 403 Missing or insufficient permissions."
     ]
    }
   ],
   "source": [
    "# create good and bad subject lists (based on completion)\n",
    "good_id_master_list = []\n",
    "bad_id_master_list = []\n",
    "\n",
    "for group in groups:\n",
    "    good_id_list = [] # stores participants who completed HIT\n",
    "    bad_id_list = [] # stores participants who started but didn't complete HIT\n",
    "    sub_list = []\n",
    "\n",
    "    group_path = f'{experiment}/subjects/{group}'\n",
    "    group_collection = db.collection(group_path)\n",
    "    group_subs = group_collection.stream()\n",
    "    \n",
    "    for sub in group_subs:\n",
    "        print(\"hello\")\n",
    "        sub_dict = sub.to_dict()\n",
    "\n",
    "        if 'currentState' in sub_dict: # check to see if subject even started HIT\n",
    "            if sub_dict['currentState'] == 'debrief' or 'HIT_complete' in sub_dict: # only keep subs who finished\n",
    "                good_id_list.append(sub.id)\n",
    "                good_id_master_list.append(sub.id)\n",
    "                curr = pd.Series(sub_dict)\n",
    "                sub_list.append(curr)\n",
    "                file_path = f'{local_folder}/Subjects/{sub.id}.csv'\n",
    "                curr.to_csv(file_path)\n",
    "        else:\n",
    "            bad_id_list.append(sub.id)\n",
    "            bad_id_master_list.append(sub.id)\n",
    "            curr = pd.Series(sub_dict)\n",
    "            file_path = f'{local_folder}/Subjects/Incomplete/{sub.id}.csv'\n",
    "            curr.to_csv(file_path)\n",
    "\n",
    "    group_df = pd.DataFrame(sub_list)\n",
    "    group_df.to_csv(f'{local_folder}/Subjects/Groups/{group}.csv')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all ratings from subjects who completed task and store locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loops over subjects and gets ratings from database to store in local dictionary\n",
    "# also counts number of subjects that have rated each question\n",
    "# links subjects to movies and movies to subjects\n",
    "good_id_set = set(good_id_master_list) # removes repeats\n",
    "\n",
    "for good_id in good_id_set: \n",
    "    sub_to_mov[good_id] = []\n",
    "    movie_list = []\n",
    "    collection_path = f'{experiment}/ratings/{good_id}'\n",
    "    curr_sub_ratings = db.collection(collection_path)\n",
    "    HITs = curr_sub_ratings.stream()\n",
    "\n",
    "    curr_movie_dict = {}\n",
    "    for HIT in HITs:\n",
    "        sub_to_mov[good_id].append(HIT.id)\n",
    "        mov_to_sub[HIT.id].append(good_id)\n",
    "        curr_movie_dict[HIT.id] = HIT.to_dict()\n",
    "        movie_q_count_dict[HIT.id] += 1\n",
    "\n",
    "    if good_id in subject_rating_dict:\n",
    "        updated = subject_rating_dict[good_id].append(curr_movie_dict)\n",
    "        subject_rating_dict[good_id] = updated\n",
    "    else:\n",
    "        movie_list.append(curr_movie_dict)\n",
    "        subject_rating_dict[good_id] = movie_list\n",
    "\n",
    "movie_counts = pd.Series(movie_q_count_dict)\n",
    "movie_counts.to_csv(f'{local_folder}/Summary/Movie_Counts.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write out rating dictionary to .csv rating files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writes file containing rating-timestamp pairs for each subject to folder for each movie-rating pairing\n",
    "for good_id in good_id_set:\n",
    "    curr_sub = subject_rating_dict[good_id]\n",
    "    for dictionary in curr_sub:\n",
    "        for movie_rating in dictionary:\n",
    "            words = movie_rating.split('-')\n",
    "            blank_mov_pd = blank_pd_dict[words[0]].copy()\n",
    "            rating_dict = dictionary[movie_rating]\n",
    "            for timestamp in rating_dict:\n",
    "                blank_mov_pd.iloc[int(timestamp)] = rating_dict[timestamp]  \n",
    "            file_path = f'{local_folder}/Ratings/{movie_rating}/{good_id}.csv'\n",
    "            blank_mov_pd.to_csv(file_path)      "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine subject and rating info together to create long format file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the base frame for appending\n",
    "cols = ['workerId', 'movie', 'ratingType', 'HIT_complete', 'age', 'assignmentId', 'birth', 'consentStatus', 'currentState', \\\n",
    "'ethnicity', 'feedback', 'handed', 'hitId', 'nativeLang', 'race', 'sex', 'startTime', \\\n",
    "'userId', 'mostRecentTime', 'timeStamp', 'ratingScore']\n",
    "\n",
    "base_frame = pd.DataFrame(np.nan, index=[0], columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this take individual local csvs and turns them into long format per movie per rating per subject\n",
    "dir_list = glob.glob(f'{local_folder}/Ratings/*')\n",
    "master_long = base_frame.copy()\n",
    "\n",
    "for directory in dir_list:\n",
    "    path = directory + '/*.csv'\n",
    "    rating_list = glob.glob(path)\n",
    "    for file in rating_list:\n",
    "        sub_id = os.path.basename(file).split('.')[0]\n",
    "        movie_rating = file.split('/')[2]\n",
    "        movie = movie_rating.split('-')[0]\n",
    "        rating = movie_rating.split('-')[1]       \n",
    "\n",
    "        # this should check and not rewrite files that already exist (speeds up process)\n",
    "        if not os.path.isfile(f'{local_folder}/Long/{movie}-{rating}-{sub_id}.csv'):\n",
    "            subject_long = base_frame.copy()\n",
    "            rating_pd = pd.read_csv(file)\n",
    "            sub_path = f'{local_folder}/Subjects/{sub_id}.csv'\n",
    "            if os.path.isfile(sub_path):\n",
    "                sub_pd = pd.read_csv(sub_path)\n",
    "                title_list = sub_pd['Unnamed: 0'].values\n",
    "                rename_dict = {}\n",
    "                counter = 0\n",
    "                for title in title_list:\n",
    "                    rename_dict[counter] = title\n",
    "                    counter += 1   \n",
    "                new_sub_pd = sub_pd.transpose().rename(columns=rename_dict).drop(['Unnamed: 0'])\n",
    "\n",
    "                new_pd = base_frame.copy()\n",
    "                new_pd['movie'] = movie\n",
    "                new_pd['ratingType'] = rating\n",
    "\n",
    "                for category in base_frame:\n",
    "                    if category in new_sub_pd:\n",
    "                        new_pd[category] = new_sub_pd[category].values\n",
    "\n",
    "                timestamp_dict = rating_pd.transpose().drop(['Unnamed: 0'])\n",
    "                copy_pd = new_pd.copy()\n",
    "                prevScore = -1\n",
    "                rating_counter = 0\n",
    "                for timestamp in timestamp_dict:\n",
    "                    ratingScore = timestamp_dict[timestamp].values\n",
    "                    if ratingScore != -1:\n",
    "                        prevScore = ratingScore\n",
    "                    else:\n",
    "                        ratingScore = prevScore\n",
    "\n",
    "                    copy_pd['timeStamp'] = timestamp\n",
    "                    copy_pd['ratingScore'] = ratingScore\n",
    "                    subject_long = pd.concat([subject_long, copy_pd], ignore_index=True)\n",
    "\n",
    "                subject_long = subject_long.drop([0])\n",
    "                subject_long.to_csv(f'{local_folder}/Long/{movie}-{rating}-{sub_id}.csv')                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this appends all individual long format files into one giant long format panda\n",
    "master_long = base_frame.copy()\n",
    "long_list = glob.glob(f'{local_folder}/Long/*.csv')\n",
    "\n",
    "for file in long_list:\n",
    "    curr_pd = pd.read_csv(file)\n",
    "    master_long = pd.concat([master_long, curr_pd])\n",
    "\n",
    "master_long.drop([0])\n",
    "master_long.to_csv(f'{local_folder}/master_long.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in saved copy\n",
    "master_long = pd.read_csv(f'{local_folder}/master_long.csv')\n",
    "master_long.drop(labels=[0], inplace=True)\n",
    "master_long.drop(labels=['Unnamed: 0', 'Unnamed: 0.1'], axis=1, inplace=True)\n",
    "sub_list = master_long['workerId'].unique()\n",
    "movie_list = master_long['movie'].unique()\n",
    "rating_list = master_long['ratingType'].unique()\n",
    "movie = movie_list[0]\n",
    "rating = rating_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_long"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
